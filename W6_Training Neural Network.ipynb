{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "[View in Colaboratory](https://colab.research.google.com/github/cheonbok94/DeepLearningLecture/blob/master/Training_Neural_Network.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "a4TOMyEqB9St"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtorch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl is not a supported wheel on this platform.\u001b[0m\r\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-143c2d8eef76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.3.0.post4-{platform}-linux_x86_64.whl torchvision'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# http://pytorch.org/\n",
    "from os import path\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "\n",
    "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
    "\n",
    "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.3.0.post4-{platform}-linux_x86_64.whl torchvision\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "4oOxnJsphGb3"
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn #\n",
    "import torch.nn.functional as F #\n",
    "import torchvision # 이미지 관련 처리, Pretrained Model 관련된 Package 입니다. \n",
    "import torchvision.datasets as vision_dsets\n",
    "import torchvision.transforms as T # 이미지 처리 (Vison) 관련된 transformation이 정의 되어 있습니다.\n",
    "import torch.optim as optim # pytorch 에서 정의한 수 많은 optimization function 들이 들어 있습니다.\n",
    "from torch.autograd import Variable \n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9ABEFSDDZ8ON"
   },
   "source": [
    "# MNIST Feed-forward Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qije6EXLt9sZ"
   },
   "source": [
    "## Data Loader 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Yo1YYiybty5e"
   },
   "outputs": [],
   "source": [
    "\n",
    "def MNIST_DATA(root='./',train =True,transforms=None ,download =True,batch_size = 32,num_worker = 1):\n",
    "\n",
    "\tprint (\"[+] Get the MNIST DATA\")\n",
    "\t\"\"\"\n",
    "  \ttorchvision.dataset 에는 우리가 많이 사용하는 데이터들을 쉽게 사용할 수 있도록 되어 있습니다. \n",
    "  \tMachine Learning 에서 Hello world 라고 불리는 Mnist 데이터를 사용해 보겠습니다. \n",
    "  \n",
    "  \n",
    "\t\"\"\"\n",
    "\tmnist_train = vision_dsets.MNIST(root = root,  #root 는 데이터의 저장 위치 입니다. \n",
    "\t\t\t\t\t\t\t\t\ttrain = True, #Train 은 이 데이터가 train 데이터인지 아닌지에 대한 정보입니다. \n",
    "\t\t\t\t\t\t\t\t\ttransform = T.ToTensor(), # 얻어낸 데이터를 pytorch가 계산 할 수 있는 Tensor 로 변환해 줍니다. \n",
    "\t\t\t\t\t\t\t\t\tdownload = True)  # 데이터를 다운로드 할지 여부를 물어봅니다. \n",
    "\tmnist_test = vision_dsets.MNIST(root = root,\n",
    "\t\t\t\t\t\t\t\t\ttrain = False,  # Test Data를 가져오기에 Train =False 를 줘야 합니다. \n",
    "\t\t\t\t\t\t\t\t\ttransform = T.ToTensor(),\n",
    "\t\t\t\t\t\t\t\t\tdownload = True)\n",
    "\t\"\"\"\n",
    "  \tData Loader 는 데이터와 batch size의 정보를 바탕으로 매 iteration 마다 주어진 데이터를 원하는 batch size 만큼 반환해주는 iterator입니다. \n",
    "  \t* Practical Guide : Batch size 는 어느정도가 좋나요? -- 클 수록 좋다는 소리가 있습니다. 하지만 gpu memeory 사이즈 한계에 의해 기본적으로 batch size 가 \n",
    "  \t커질 수록 학습에 사용되는 gpu memory 사이즈가 큽니다. (Activation map을 저장해야 하기 때문입니다.) 기본적으로 2의 배수로 저장하는 것이 좋습니다.(Bit size 관련) \n",
    "  \n",
    "\t\"\"\"\n",
    "\ttrainDataLoader = data.DataLoader(dataset = mnist_train,  # DataSet은 어떤 Data를 제공해 줄지에 대한 정보입니다. 여기서는 Training DATA를 제공합니다. \n",
    "\t\t\t\t\t\t\t\t\tbatch_size = batch_size, # batch size 정보를 꼭 줘야 합니다. 한 Batch 당 몇 개의 Data 를 제공할지에 대한 정보입니다. \n",
    "\t\t\t\t\t\t\t\t\tshuffle =True, # Training의 경우 Shuffling 을 해주는 것이 성능에 지대한 영향을 끼칩니다. 꼭 True 를 줘야 합니다. \n",
    "\t\t\t\t\t\t\t\t\tnum_workers = 1) # num worker의 경우 데이터를 로드하는데 worker를 얼마나 추가하겠는가에 대한 정보입니다. \n",
    "\n",
    "\ttestDataLoader = data.DataLoader(dataset = mnist_test, # Test Data Loader 이므로 Test Data를 인자로 전달해줍니다.\n",
    "\t\t\t\t\t\t\t\t\tbatch_size = batch_size, # 마찬가지로 Batch size 를 넣어줍니다. \n",
    "\t\t\t\t\t\t\t\t\tshuffle = False, # shuffling 이 굳이 필요하지 않으므로 false를 줍니다. \n",
    "\t\t\t\t\t\t\t\t\tnum_workers = 1) #\n",
    "\tprint (\"[+] Finished loading data & Preprocessing\")\n",
    "\treturn mnist_train,mnist_test,trainDataLoader,testDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1850,
     "status": "ok",
     "timestamp": 1529991740843,
     "user": {
      "displayName": "박기태",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "106295349720515296487"
     },
     "user_tz": -540
    },
    "id": "_P2oJog_xnFJ",
    "outputId": "5fde5deb-c0c1-4561-f3be-53017f12f967"
   },
   "outputs": [],
   "source": [
    "trainDset,testDset,trainDataLoader,testDataLoader= MNIST_DATA(batch_size = 32)  # Data Loader 를 불러 옵니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vtWA2FTIxskN"
   },
   "source": [
    "## Train Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "PA8glX3ox0Hh"
   },
   "outputs": [],
   "source": [
    "def train_network(net,optimizer,trainloader):\n",
    "  for epoch in range(4):  # loop over the dataset multiple times\n",
    "\n",
    "      running_loss = 0.0 # running loss를 저장하기 위한 변수입니다. \n",
    "      for i, data in enumerate(trainloader, 0): # 한 Epoch 만큼 돕니다. 매 iteration 마다 정해진 Batch size 만큼 데이터를 뱉습니다. \n",
    "          # get the inputs\n",
    "          inputs, labels = data # DataLoader iterator의 반환 값은 input_data 와 labels의 튜플 형식입니다. \n",
    "          inputs = Variable(inputs).cuda() # Pytorch에서 nn.Module 에 넣어 Backprop을 계산 하기 위해서는 Variable로 감싸야 합니다.\n",
    "          labels = Variable(labels).cuda()\n",
    "          # zero the parameter gradients\n",
    "          optimizer.zero_grad()    #  현재 기존의 backprop을 계산하기 위해서 저장했던 activation buffer 를 비웁니다. Q) 이걸 안 한다면?\n",
    "\n",
    "          # forward + backward + optimize\n",
    "          outputs = net(inputs) # input 을 넣은 위 network 로 부터 output 을 얻어냅니다. \n",
    "          loss = criterion(outputs, labels) # loss fucntion에 주어진 target과 output 의 score를 계산하여 반환합니다. \n",
    "          loss.backward() # * Scalar Loss value를 Backward() 해주게 되면 주어진 loss값을 바탕으로 backpropagation이 진행됩니다. \n",
    "          optimizer.step() # 계산된 Backprop 을 바탕으로 optimizer가 gradient descenting 을 수행합니다. \n",
    "\n",
    "          # print statistics\n",
    "          running_loss += loss.data[0]\n",
    "          if i % 500 == 499:    # print every 2000 mini-batches\n",
    "              print('[%d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / 500))\n",
    "              running_loss = 0.0\n",
    "\n",
    "  print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zGMkGKQiy8_r"
   },
   "source": [
    "## Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "X0Xn-H2zbc22"
   },
   "outputs": [],
   "source": [
    "def test(model,test_loader):\n",
    "  model.eval() # Eval Mode 왜 해야 할까요?  --> nn.Dropout BatchNorm 등의 Regularization 들이 test 모드로 들어가게 되기 때문입니다. \n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  for data, target in test_loader:\n",
    "    data, target = Variable(data).cuda(), Variable(target).cuda()  # 기존의 train function의 data 처리부분과 같습니다. \n",
    "    output = model(data) \n",
    "    pred = output.max(1, keepdim=True)[1] # get the index of the max \n",
    "    correct += pred.eq(target.view_as(pred)).sum().data[0] # 정답 데이터의 갯수를 반환합니다. \n",
    "\n",
    "  test_loss /= len(test_loader.dataset)\n",
    "  print('\\nTest set:  Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "      correct, len(test_loader.dataset),\n",
    "      100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mdM-bsXD152f"
   },
   "source": [
    "## Neural Network  + Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fmv3W0Ln29M1"
   },
   "source": [
    "### 간단한 Neural Network 를 만들어 봅시다. (1)\n",
    "특징 : 2개의 Layer를 가지는 Neural Network \n",
    "<구성>  \n",
    "Layer 1 - input:28*28 , output : 30 + Activation Fucntion - Sigmoid \n",
    "\n",
    "Layer 2 - input: 30 output:10\n",
    "\n",
    "Cross Entropy Loss  + SGD optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "MTJOsz9U2FnP"
   },
   "outputs": [],
   "source": [
    "\n",
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__() # nn.Module 생성자 호출 Q) 왜 필요할까요?\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc0 = nn.Linear(28*28,30)\n",
    "        self.fc1 = nn.Linear(30, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,28*28) # x.view함수는 주어진 인자의 크기로 해당 데이터의 크기를 반환합니다. 즉, (Batch_size,28,28) --> (Batch_size,28*28)로 변환합니다.\n",
    "        x = ??(self.fc0(x)) # 28*28 -> 30 -> Activation function 을 수행합니다.\n",
    "        x = self.fc1(x)  # 30 -> 10 으로 10개의 Class에 대한 logit 값을 호출합니다. \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bvk4TTL83pcc"
   },
   "source": [
    "#### Optimizer \n",
    "Optimizer 의 경우 기본적으로 torch.optim 안에 존재합니다. 다양한 optimziers 가 정의되어 있습니다. \n",
    "\n",
    "기본적으로 다음과 같은 구성을 따릅니다. optim.{Optimzier 이름}({Network Parameters},lr ={learning rate })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "wd5tmyMnXcs0"
   },
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().cuda() # 생성한 뉴럴넷 Instance를 생성하고 빠른 학습을 위해 cuda 에 올립니다. \n",
    "criterion = nn.CrossEntropyLoss() # Loss Function을 정의 합니다. 여기서는 cross entrophy loss 를 사용합니다. \n",
    "optimizer = optim.SGD(mnist_net.parameters(), lr=0.001) # optimizer는 이와 같이 training 할 Parameter와 learning rate를 인자로 줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "zALyUje-aehn"
   },
   "outputs": [],
   "source": [
    "train_network(mnist_net,optimizer,trainDataLoader) # 4 Epoch 정도 학습을 진행해봅니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "LfQrMIX2a3nn"
   },
   "outputs": [],
   "source": [
    "test(mnist_net,testDataLoader) # Test 정확도를 출력해 봅니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DUK2GbHZ4x-K"
   },
   "source": [
    "### 간단한 Neural Network 를 만들어 봅시다. (2)\n",
    "특징 : 2개의 Layer를 가지는 Neural Network \n",
    "<구성>  \n",
    "Layer 1 - input:28*28 , output : 30 + Activation Fucntion - tanh \n",
    "\n",
    "Layer 2 - input: 30 output:10\n",
    "\n",
    "Cross Entropy Loss  + SGD optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 852,
     "status": "ok",
     "timestamp": 1529915573833,
     "user": {
      "displayName": "Keetae park",
      "photoUrl": "//lh4.googleusercontent.com/-HRf48e0fZvg/AAAAAAAAAAI/AAAAAAAAARU/UAGYNn-FRto/s50-c-k-no/photo.jpg",
      "userId": "112755870975618660481"
     },
     "user_tz": -540
    },
    "id": "FZUmc76ZdOap",
    "outputId": "e5c007c2-9cf6-4beb-a7e5-489f6f3af1cc"
   },
   "outputs": [],
   "source": [
    "\n",
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__() # nn.Module 생성자 호출 Q) 왜 필요할까요?\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc0 = nn.Linear(28*28,30)\n",
    "        self.fc1 = nn.Linear(30, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,28*28) # x.view함수는 주어진 인자의 크기로 해당 데이터의 크기를 반환합니다. 즉, (Batch_size,28,28) --> (Batch_size,28*28)로 변환합니다.\n",
    "        x = ??(self.fc0(x)) # 28*28 -> 30 -> Activation function 을 수행합니다.\n",
    "        x = self.fc1(x)  # 30 -> 10 으로 10개의 Class에 대한 logit 값을 호출합니다. \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 610,
     "status": "ok",
     "timestamp": 1529915575205,
     "user": {
      "displayName": "Keetae park",
      "photoUrl": "//lh4.googleusercontent.com/-HRf48e0fZvg/AAAAAAAAAAI/AAAAAAAAARU/UAGYNn-FRto/s50-c-k-no/photo.jpg",
      "userId": "112755870975618660481"
     },
     "user_tz": -540
    },
    "id": "UBXketXnhA_E",
    "outputId": "37c4f403-d8f1-4b05-c8ef-4cc5eed9addb"
   },
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().cuda() # 생성한 뉴럴넷 Instance를 생성하고 빠른 학습을 위해 cuda 에 올립니다. \n",
    "criterion = nn.CrossEntropyLoss() # Loss Function을 정의 합니다. 여기서는 cross entrophy loss 를 사용합니다. \n",
    "optimizer = optim.SGD(mnist_net.parameters(), lr=0.001) # optimizer는 이와 같이 training 할 Parameter와 learning rate를 인자로 줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Qb3e4wqphDmZ"
   },
   "outputs": [],
   "source": [
    "train_network(mnist_net,optimizer,trainDataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ZJ-WCpKrhEzR"
   },
   "outputs": [],
   "source": [
    "test(mnist_net,testDataLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IV_aGry05TIV"
   },
   "source": [
    "### 간단한 Neural Network 를 만들어 봅시다. (3)\n",
    "특징 : 2개의 Layer를 가지는 Neural Network \n",
    "<구성>  \n",
    "Layer 1 - input:28*28 , output : 30 + Activation Fucntion - Relu\n",
    "\n",
    "Layer 2 - input: 30 output:10\n",
    "\n",
    "Cross Entropy Loss  + SGD optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "PvAhsyP-hbwx"
   },
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__()\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc0 = nn.Linear(28*28,30)\n",
    "        self.fc1 = nn.Linear(30, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = x.view(-1,28*28)\n",
    "        x = ??(self.fc0(x))\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "mZVpqqCbhiNd"
   },
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().cuda() # 생성한 뉴럴넷 Instance를 생성하고 빠른 학습을 위해 cuda 에 올립니다. \n",
    "criterion = nn.CrossEntropyLoss() # Loss Function을 정의 합니다. 여기서는 cross entrophy loss 를 사용합니다. \n",
    "optimizer = optim.SGD(mnist_net.parameters(), lr=0.001) # optimizer는 이와 같이 training 할 Parameter와 learning rate를 인자로 줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "fKGyPdSIhmqx"
   },
   "outputs": [],
   "source": [
    "train_network(mnist_net,optimizer,trainDataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "1_tEaqWlhnUQ"
   },
   "outputs": [],
   "source": [
    "test(mnist_net,testDataLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FfWtUiir7XAK"
   },
   "source": [
    "### Q) 성능차이가 존재하나요? 존재한다면 무슨 이유일까요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FQALVsvI5ful"
   },
   "source": [
    "### 간단한 Neural Network 를 만들어 봅시다. (4) \n",
    "특징 : 3개의 Layer를 가지는 Neural Network \n",
    "<구성>  \n",
    "Layer 1 - input:28*28 , output : 40 + Activation Fucntion - sigmoid \n",
    "\n",
    "Layer 2 - input: 40 output: 30\n",
    "\n",
    "Layer 3 - input: 30 output : 10\n",
    "\n",
    "Cross Entropy Loss  + SGD optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "TLEQ6RZXjOCi"
   },
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__()\n",
    "        self.fc0 = nn.Linear(28*28,??) # Layer 1\n",
    "        self.fc1 = nn.Linear(??, ??) # Layer 2\n",
    "        self.fc2 = nn.Linear(??, 10) # Layer 3\n",
    "\n",
    "    def forward(self, x):\n",
    "      \n",
    "        x = x.view(-1,28*28)\n",
    "        x = ??(self.fc0(x))\n",
    "        x = ??(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "1MM_WIY6jVG6"
   },
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().cuda() # 생성한 뉴럴넷 Instance를 생성하고 빠른 학습을 위해 cuda 에 올립니다. \n",
    "criterion = nn.CrossEntropyLoss() # Loss Function을 정의 합니다. 여기서는 cross entrophy loss 를 사용합니다. \n",
    "optimizer = optim.SGD(mnist_net.parameters(), lr=0.001) # optimizer는 이와 같이 training 할 Parameter와 learning rate를 인자로 줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "3kWON9jtjXAy"
   },
   "outputs": [],
   "source": [
    "train_network(mnist_net,optimizer,trainDataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "OaCuXaHWjXzH"
   },
   "outputs": [],
   "source": [
    "test(mnist_net,testDataLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hxqP1tY965JF"
   },
   "source": [
    "### Q) 학습이 잘 되나요???? 안 된다면 왜 안될까요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JZIkQe9f6tdy"
   },
   "source": [
    "### 간단한 Neural Network 를 만들어 봅시다. (5) \n",
    "특징 : 3개의 Layer를 가지는 Neural Network \n",
    "<구성>  \n",
    "Layer 1 - input:28*28 , output : 40 + Activation Fucntion - Relu \n",
    "\n",
    "Layer 2 - input: 40 output: 30\n",
    "\n",
    "Layer 3 - input: 30 output : 10\n",
    "\n",
    "Cross Entropy Loss  + SGD optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "0uHrf8bRjZFw"
   },
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__()\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc0 = nn.Linear(28*28,40) #Layer 1 \n",
    "        self.fc1 = nn.Linear(40, 30) # Layer 2\n",
    "        self.fc2 = nn.Linear(30, 10) # Layer 3\n",
    "\n",
    "    def forward(self, x):\n",
    "       \n",
    "        x = x.view(-1,28*28)\n",
    "        x = ??(self.fc0(x)) # Layer 1\n",
    "        x = ??(self.fc1(x)) # Layer 2\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "kEtXES71j9s6"
   },
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().cuda() # 생성한 뉴럴넷 Instance를 생성하고 빠른 학습을 위해 cuda 에 올립니다. \n",
    "criterion = nn.CrossEntropyLoss() # Loss Function을 정의 합니다. 여기서는 cross entrophy loss 를 사용합니다. \n",
    "optimizer = optim.SGD(mnist_net.parameters(), lr=0.001) # optimizer는 이와 같이 training 할 Parameter와 learning rate를 인자로 줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "IxHiSRSskasK"
   },
   "outputs": [],
   "source": [
    "train_network(mnist_net,optimizer,trainDataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "0woJ7sbzkbqj"
   },
   "outputs": [],
   "source": [
    "test(mnist_net,testDataLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5YCjGFh77xeq"
   },
   "source": [
    "#### (4)와 차이가 존재하나요? 그렇다면 왜 그럴까요? (3) 이랑은 비교해보면 어떻나요? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ezH4C21Y8JF4"
   },
   "source": [
    "### 간단한 Neural Network 를 만들어 봅시다. (6) \n",
    "특징 : 3개의 Layer를 가지는 Neural Network \n",
    "<구성>  \n",
    "Layer 1 - input:28*28 , output : 40 + Activation Fucntion - Relu \n",
    "\n",
    "Layer 2 - input: 40 output: 30\n",
    "\n",
    "Layer 3 - input: 30 output : 10\n",
    "\n",
    "Cross Entropy Loss  + **Adam** optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "l-oDSJBL8Wk5"
   },
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__()\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc0 = nn.Linear(28*28,40) #Layer 1 \n",
    "        self.fc1 = nn.Linear(40, 30) # Layer 2\n",
    "        self.fc2 = nn.Linear(30, 10) # Layer 3\n",
    "\n",
    "    def forward(self, x):\n",
    "       \n",
    "        x = x.view(-1,28*28)\n",
    "        x = F.relu(self.fc0(x)) # Layer 1\n",
    "        x = F.relu(self.fc1(x)) # Layer 2\n",
    "        x = self.fc2(x) # Layer 3 \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "EJEAkxqX8YWR"
   },
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().cuda() # 생성한 뉴럴넷 Instance를 생성하고 빠른 학습을 위해 cuda 에 올립니다. \n",
    "criterion = nn.CrossEntropyLoss() # Loss Function을 정의 합니다. 여기서는 cross entrophy loss 를 사용합니다. \n",
    "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001) # optimizer는 이와 같이 training 할 Parameter와 learning rate를 인자로 줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "colab_type": "code",
    "id": "AVGBPs3W8dRd",
    "outputId": "089193da-dbd1-4957-8ce4-15a3be1a1c5a"
   },
   "outputs": [],
   "source": [
    "train_network(mnist_net,optimizer,trainDataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "PKcDCwPJ8dqP",
    "outputId": "808e3b59-1787-4bc9-eb83-7366d4b48e01"
   },
   "outputs": [],
   "source": [
    "test(mnist_net,testDataLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P7_NK6ue8t6z"
   },
   "source": [
    "### 간단한 Neural Network 를 만들어 봅시다. (7) Layer 를 줄여볼까요? \n",
    "특징 : 2개의 Layer를 가지는 Neural Network \n",
    "<구성>  \n",
    "Layer 1 - input:28*28 , output : 30 + Activation Fucntion - Relu \n",
    "\n",
    "Layer 2 - input: 30 output : 10\n",
    "\n",
    "Cross Entropy Loss  + **Adam** optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "QjnnxkiG8slU"
   },
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__()\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc0 = nn.Linear(28*28,30) #Layer 1 \n",
    "        self.fc1 =  nn.Linear(30, 10) # Layer 2\n",
    "\n",
    "    def forward(self, x):\n",
    "       \n",
    "        x = x.view(-1,28*28)\n",
    "        x = F.relu(self.fc0(x)) # Layer 1\n",
    "        x = self.fc1(x) # Layer 2\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "nQea1DZS8s2E"
   },
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().cuda() # 생성한 뉴럴넷 Instance를 생성하고 빠른 학습을 위해 cuda 에 올립니다. \n",
    "criterion = nn.CrossEntropyLoss() # Loss Function을 정의 합니다. 여기서는 cross entrophy loss 를 사용합니다. \n",
    "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001) # optimizer는 이와 같이 training 할 Parameter와 learning rate를 인자로 줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "colab_type": "code",
    "id": "-wPfgH1S8tFP",
    "outputId": "8ab31da8-77e1-43a4-f6ab-23280d4bfde8"
   },
   "outputs": [],
   "source": [
    "train_network(mnist_net,optimizer,trainDataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "_8vW8d-18-Ix",
    "outputId": "a506c72b-9224-46f3-efc8-879ab1b75b0c"
   },
   "outputs": [],
   "source": [
    "test(mnist_net,testDataLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nKbhGSix9UQM"
   },
   "source": [
    "### 간단한 Neural Network 를 만들어 봅시다. (7) Batch Norm 을 줘 볼까요?\n",
    "특징 : 2개의 Layer를 가지는 Neural Network \n",
    "<구성>  \n",
    "Layer 1 - input:28*28 , output : 30 + Activation Fucntion - Relu  + Batch Norm\n",
    "\n",
    "Layer 2 - input: 30 output : 10\n",
    "\n",
    "Cross Entropy Loss  + **Adam** optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "rpakqWPv9cs0"
   },
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__()\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc0 = nn.Linear(28*28,30) #Layer 1\n",
    "        self.bn0 = nn.BatchNorm1d(30) # BatchNorm \n",
    "        self.fc1 =  nn.Linear(30, 10) # Layer 2\n",
    "    def forward(self, x):\n",
    "       \n",
    "        x = x.view(-1,28*28)\n",
    "        x = F.relu(self.bn0(self.fc0(x))) # Layer 1\n",
    "        x = self.fc1(x) # Layer 2\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "suyt84BXHzE9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "VHANHwQT9c6N"
   },
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().cuda() # 생성한 뉴럴넷 Instance를 생성하고 빠른 학습을 위해 cuda 에 올립니다. \n",
    "criterion = nn.CrossEntropyLoss() # Loss Function을 정의 합니다. 여기서는 cross entrophy loss 를 사용합니다. \n",
    "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001) # optimizer는 이와 같이 training 할 Parameter와 learning rate를 인자로 줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "colab_type": "code",
    "id": "el8EvYky9dgz",
    "outputId": "296307dd-2dce-4008-b6fa-266857dbb2cf"
   },
   "outputs": [],
   "source": [
    "train_network(mnist_net,optimizer,trainDataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "IIUIeYYe9rid",
    "outputId": "6d5f94e9-7a20-4a33-f6d5-69baa02f6c4b"
   },
   "outputs": [],
   "source": [
    "test(mnist_net,testDataLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mBTuBB9_93mq"
   },
   "source": [
    "### 간단한 Neural Network 를 만들어 봅시다. (8) 더 깊은 레이어에 Batch Norm 을 줘 볼까요?\n",
    "특징 : 2개의 Layer를 가지는 Neural Network\n",
    "\n",
    "<구성>  \n",
    "\n",
    "Layer 1 - input:28*28 , output : 40 + Activation Fucntion - Relu + BatchNorm\n",
    "\n",
    "Layer 2 - input: 40 output: 30 + Activation Fucntion - Relu  + BatchNorm\n",
    "\n",
    "Layer 3 - input: 30 output : 10\n",
    "\n",
    "Cross Entropy Loss  + **Adam** optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "qgGaaFW8-Ny7"
   },
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__()\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc0 = nn.Linear(28*28,40) #Layer 1 \n",
    "        self.bn0 = nn.BatchNorm1d(40) #BatchNorm1 \n",
    "        self.fc1 = nn.Linear(40, 30) # Layer 2\n",
    "        self.bn1 = nn.BatchNorm1d(30) #BatchNorm1 \n",
    "        self.fc2 = nn.Linear(30, 10) # Layer 3\n",
    "\n",
    "    def forward(self, x):\n",
    "       \n",
    "        x = x.view(-1,28*28)\n",
    "        x = F.relu(self.bn0(self.fc0(x))) # Layer 1\n",
    "        x = F.relu(self.bn1(self.fc1(x))) # Layer 2\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ClLHQwZE-OCZ"
   },
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().cuda() # 생성한 뉴럴넷 Instance를 생성하고 빠른 학습을 위해 cuda 에 올립니다. \n",
    "criterion = nn.CrossEntropyLoss() # Loss Function을 정의 합니다. 여기서는 cross entrophy loss 를 사용합니다. \n",
    "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001) # optimizer는 이와 같이 training 할 Parameter와 learning rate를 인자로 줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "colab_type": "code",
    "id": "KxHuM-IF-nz2",
    "outputId": "2337bee3-ddf3-46de-a931-685d518b2b12"
   },
   "outputs": [],
   "source": [
    "train_network(mnist_net,optimizer,trainDataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "xK1kspbG-qB9",
    "outputId": "f3ca70b4-dc8f-4b28-aac6-a45c36ee1fce"
   },
   "outputs": [],
   "source": [
    "test(mnist_net,testDataLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xxghrniJ_JIm"
   },
   "source": [
    "#### Batch Normalization 을 적용한 (7)과 (6)을 비교해보고 (8) 과 (5)를 비교해보면 어떻나요? 학습이 어떻게 달라졌을까요? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vGwSFl9v_c4F"
   },
   "source": [
    "### Let's Do it - 성능을 한번 끝까지 높여볼까요~? 마음대로 한번 최고 성능을 찍어봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "fjM6p0gvkccx"
   },
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__()\n",
    "        \n",
    "        self.fc0 = nn.Linear(28*28,16*5*5)\n",
    "        self.bn0 = nn.BatchNorm1d(16*5*5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 84)\n",
    "        self.bn1 = nn.BatchNorm1d(84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "      \n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = x.view(-1,28*28)\n",
    "        x = F.relu(self.bn0(self.fc0(x)))\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "qGsCsD6blqBt"
   },
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "3wPzE5N0lq-Y"
   },
   "outputs": [],
   "source": [
    "train_network(mnist_net,optimizer,trainDataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "bhuKrwYElsAj"
   },
   "outputs": [],
   "source": [
    "test(mnist_net,testDataLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xxip6TQc_txE"
   },
   "source": [
    "## Practical Guide Pytorch nn.Sequential "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SDexfQHd_72G"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "x = F.relu(self.bn0(self.fc0(x)))\n",
    "x = F.relu(self.bn1(self.fc1(x)))\n",
    "```\n",
    "너무 복잡하지 않나요?  그냥 x = self.fc(x) 쉽게 해버리면 안 될까요?\n",
    "\n",
    "Solution : nn.Sequential + 자매품 nn.ModuList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "59l4rpz5ls39"
   },
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__()\n",
    "        \n",
    "        layer_list = [] # 이 리스트에 모든 Layer 를 순차적으로 append 해보겠습니다. \n",
    "        layer_list.append(nn.Linear(28*28,40)) #Layer 1 \n",
    "        layer_list.append(nn.BatchNorm1d(40))#BatchNorm1 \n",
    "        layer_list.append(nn.Linear(40, 30)) # Layer 2\n",
    "        layer_list.append(nn.BatchNorm1d(30)) #BatchNorm1 \n",
    "        layer_list.append(nn.Linear(30, 10)) # Layer 3\n",
    "        self.net  = nn.Sequential(*layer_list) # nn.Sequential 에 layer list를 넘겨 줍니다.\n",
    "    def forward(self, x):\n",
    "       \n",
    "        x = x.view(-1,28*28)\n",
    "        x = self.net(x) # 넣은 순서대로 적용이 됩니다. \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "NRRIS8y1mOOK"
   },
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().cuda() # 생성한 뉴럴넷 Instance를 생성하고 빠른 학습을 위해 cuda 에 올립니다. \n",
    "criterion = nn.CrossEntropyLoss() # Loss Function을 정의 합니다. 여기서는 cross entrophy loss 를 사용합니다. \n",
    "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001) # optimizer는 이와 같이 training 할 Parameter와 learning rate를 인자로 줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "CF5Tr8dXBMmg"
   },
   "outputs": [],
   "source": [
    "train_network(mnist_net,optimizer,trainDataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2101,
     "status": "ok",
     "timestamp": 1529995121267,
     "user": {
      "displayName": "박기태",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "106295349720515296487"
     },
     "user_tz": -540
    },
    "id": "b7o8XOAlBSFB",
    "outputId": "f03e2034-e7e8-4dfc-cd4a-3c974c2a4f61"
   },
   "outputs": [],
   "source": [
    "test(mnist_net,testDataLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ysv7uX-1Dh0b"
   },
   "source": [
    "#### 연습해 봅시다 ! \n",
    "\n",
    "특징 : 2개의 Layer를 가지는 Neural Network <구성>\n",
    "\n",
    "Layer 1 - input:28*28 , output : 30 + Activation Fucntion - Relu + Batch Norm\n",
    "\n",
    "Layer 2 - input: 30 output : 10\n",
    "\n",
    "Cross Entropy Loss + Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "-esUcJ8VDu_q"
   },
   "outputs": [],
   "source": [
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_Net, self).__init__()\n",
    "        \n",
    "        layer_list = [] # 이 리스트에 모든 Layer 를 순차적으로 append 해보겠습니다. \n",
    "        layer_list.append(nn.Linear(28*28,30)) #Layer 1 \n",
    "        layer_list.append(nn.BatchNorm1d(30)) #BatchNorm1 \n",
    "        layer_list.append(nn.Linear(30, 10)) # Layer 2\n",
    "        self.net  = nn.Sequential(*layer_list) # nn.Sequential 에 layer list를 넘겨 줍니다.\n",
    "    def forward(self, x):\n",
    "       \n",
    "        x = x.view(-1,28*28)\n",
    "        x = self.net(x) # 넣은 순서대로 적용이 됩니다. \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "jaktO6PvD0M4"
   },
   "outputs": [],
   "source": [
    "mnist_net = MNIST_Net().cuda() # 생성한 뉴럴넷 Instance를 생성하고 빠른 학습을 위해 cuda 에 올립니다. \n",
    "criterion = nn.CrossEntropyLoss() # Loss Function을 정의 합니다. 여기서는 cross entrophy loss 를 사용합니다. \n",
    "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001) # optimizer는 이와 같이 training 할 Parameter와 learning rate를 인자로 줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "colab_type": "code",
    "id": "9She_4cMD1ES",
    "outputId": "0146b8cf-77b0-491f-c5f1-68b11a03f6d3"
   },
   "outputs": [],
   "source": [
    "train_network(mnist_net,optimizer,trainDataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "kseseeW4D1zc",
    "outputId": "ccccb43b-17f0-4da2-aac3-e463a65901b3"
   },
   "outputs": [],
   "source": [
    "test(mnist_net,testDataLoader)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "default_view": {},
   "name": "Training_Neural_Network.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
