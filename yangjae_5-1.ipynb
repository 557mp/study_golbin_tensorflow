{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실습에 필요한 라이브러리를 불러옵니다.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 결과를 출력하기 위한 엘리스 유틸리티 툴을 불러옵니다.\n",
    "import elice_utils\n",
    "eu = elice_utils.EliceUtils()\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# 데이터를 불러옵니다.\n",
    "datafile = open('data/spambase.data', 'r')\n",
    "data = []\n",
    "for line in datafile:\n",
    "    line = [float(element) for element in line.rstrip('\\n').split(',')]\n",
    "    data.append(np.asarray(line))\n",
    "\n",
    "num_features = 48\n",
    "# feature와 label을 구분해줍니다.\n",
    "X = [data[i][:num_features] for i in range(len(data))]\n",
    "y = [int(data[i][-1]) for i in range(len(data))]\n",
    "\n",
    "# train, test 데이터셋으로 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state=42)\n",
    "\n",
    "class Naive_Bayes_Classifier(object):\n",
    "    # 48개의 feature를 이용합니다\n",
    "    def __init__(self, num_features=48):\n",
    "        self.num_features = num_features\n",
    "\n",
    "    # (2) log(P(feature_vector | Class))를 계산합니다.\n",
    "    def log_likelihood_naivebayes(self, feature_vector, Class):\n",
    "        assert len(feature_vector) == self.num_features\n",
    "        log_likelihood = 0.0 #log-likelihood를 사용해 underflow 회피\n",
    "        \n",
    "        if Class == 0:\n",
    "        \n",
    "        elif Class == 1:\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Class takes integer values 0 or 1\")\n",
    "        \n",
    "        return log_likelihood\n",
    "    \n",
    "    # 3. 각 클래스의 Posterior probability를 구합니다.\n",
    "    def class_posteriors(self, feature_vector):\n",
    "        log_likelihood_ham = self.log_likelihood_naivebayes(feature_vector, Class = 0)\n",
    "        log_likelihood_spam = self.log_likelihood_naivebayes(feature_vector, Class = 1)\n",
    "        \n",
    "        log_posterior_ham = None\n",
    "        log_posterior_spam = None\n",
    "        \n",
    "        return log_posterior_ham, log_posterior_spam\n",
    "    \n",
    "    # Maximum A Priori(MAP) inference를 이용해 사후확률이 가장 큰 클래스를 선택합니다.\n",
    "    def spam_classify(self, document):\n",
    "        feature_vector = [int(element>0.0) for element in document]\n",
    "        log_posterior_ham, log_posterior_spam = self.class_posteriors(feature_vector)\n",
    "        \n",
    "        if log_posterior_ham > log_posterior_spam:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "\n",
    "    # (1)모델을 학습하는 train 함수를 작성하세요.\n",
    "    def train(self, X_train, y_train):\n",
    "        # Likelihood estimator 만들기\n",
    "        # 스팸 클래스와 햄 클래스 나누기\n",
    "        X_train_spam = [X_train[i] for i in range(len(X_train)) if y_train[i] == 1]\n",
    "        X_train_ham = [X_train[i] for i in range(len(X_train)) if y_train[i] == 0]\n",
    "    \n",
    "        # 각 클래스의 feature 각각에 대한 likelihood 구하세요.\n",
    "        self.likelihoods_ham = None\n",
    "        self.likelihoods_spam = None\n",
    "\n",
    "        # 각 class의 log-prior를 계산하세요\n",
    "        num_ham = float(len(X_train_ham))\n",
    "        num_spam = float(len(X_train_spam))\n",
    "\n",
    "        prior_probability_ham = None\n",
    "        prior_probability_spam = None\n",
    "\n",
    "        self.log_prior_ham = None\n",
    "        self.log_prior_spam = None\n",
    "    \n",
    "        return None\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        predictions = []\n",
    "        for case in X_test:\n",
    "            predictions.append(self.spam_classify(case))\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "\n",
    "NB = Naive_Bayes_Classifier()\n",
    "NB.train(X_train, y_train)\n",
    "pred = NB.predict(X_test)\n",
    "\n",
    "def evaluate_performance(predictions, ground_truth_labels):\n",
    "    correct_count = 0.0\n",
    "    for item_index in range(len(predictions)):\n",
    "        if predictions[item_index] == ground_truth_labels[item_index]:\n",
    "            correct_count += 1.0\n",
    "    accuracy = correct_count/len(predictions)\n",
    "    return accuracy\n",
    "\n",
    "accuracy_naivebayes = evaluate_performance(pred, y_test)\n",
    "print(accuracy_naivebayes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
